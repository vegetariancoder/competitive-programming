{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T22:36:59.107506Z",
     "start_time": "2024-10-25T22:36:54.380564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import substring, col, when, sum as pyspark_sum, collect_list\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars\",\n",
    "                \"/Users/sahilnagpal/Downloads/mysql-connector-java-8.0.29.jar\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"leetcode-problems\") \\\n",
    "        .getOrCreate()"
   ],
   "id": "4152a17527d25cbe",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/25 18:36:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/25 18:36:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T22:37:05.250509Z",
     "start_time": "2024-10-25T22:36:59.116132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "locations = spark.read.format(\"jdbc\"). \\\n",
    "    option(\"url\", \"jdbc:mysql://localhost:3306/leetcode\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", \"locations\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .load()\n",
    "\n",
    "locations.show()"
   ],
   "id": "3a9e3c8fbb794649",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|       state|         city|\n",
      "+------------+-------------+\n",
      "|    New York|New York City|\n",
      "|    New York|       Newark|\n",
      "|    New York|      Buffalo|\n",
      "|    New York|    Rochester|\n",
      "|  California|San Francisco|\n",
      "|  California|   Sacramento|\n",
      "|  California|    San Diego|\n",
      "|  California|  Los Angeles|\n",
      "|       Texas|        Tyler|\n",
      "|       Texas|       Temple|\n",
      "|       Texas|       Taylor|\n",
      "|       Texas|       Dallas|\n",
      "|Pennsylvania| Philadelphia|\n",
      "|Pennsylvania|   Pittsburgh|\n",
      "|Pennsylvania|    Pottstown|\n",
      "+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T22:37:07.099355Z",
     "start_time": "2024-10-25T22:37:05.302870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Create 'match_checking' column\n",
    "df_with_check = locations.withColumn(\n",
    "    \"match_checking\", \n",
    "    when(substring(col(\"state\"), 1, 1) == substring(col(\"city\"), 1, 1), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Step 2: Group by 'state', collect cities, and calculate matching count\n",
    "result_df = df_with_check.groupBy(\"state\").agg(\n",
    "    collect_list(\"city\").alias(\"cities\"),  # Group and collect cities in a list\n",
    "    pyspark_sum(\"match_checking\").alias(\"matching_letter_count\")  # Sum match_checking column\n",
    ")\n",
    "\n",
    "# Step 3: Filter for states with matching_letter_count > 0 and order by matching_letter_count in descending order\n",
    "final_df = result_df.filter(col(\"matching_letter_count\") > 0).orderBy(col(\"matching_letter_count\").desc())\n",
    "\n",
    "# Show the result\n",
    "final_df.show(truncate=False)\n"
   ],
   "id": "301385d9e59583e2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------------------------------+---------------------+\n",
      "|state       |cities                                     |matching_letter_count|\n",
      "+------------+-------------------------------------------+---------------------+\n",
      "|Texas       |[Tyler, Temple, Taylor, Dallas]            |3                    |\n",
      "|Pennsylvania|[Philadelphia, Pittsburgh, Pottstown]      |3                    |\n",
      "|New York    |[New York City, Newark, Buffalo, Rochester]|2                    |\n",
      "+------------+-------------------------------------------+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T22:37:07.159174Z",
     "start_time": "2024-10-25T22:37:07.155103Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "65fe923ee0b084d7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
