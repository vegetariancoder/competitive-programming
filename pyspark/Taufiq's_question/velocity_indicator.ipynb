{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-26T16:40:21.024201Z",
     "start_time": "2024-09-26T16:40:09.890185Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,count,row_number,max,min\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars\",\n",
    "                \"/Users/sahilnagpal/Downloads/mysql-connector-java-8.0.29.jar\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"leetcode-problems\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "auto_repair = spark.read.format(\"jdbc\"). \\\n",
    "    option(\"url\", \"jdbc:mysql://localhost:3306/leetcode\") \\\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", \"auto_repair\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .load()\n",
    "\n",
    "auto_repair.show()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/26 12:40:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/26 12:40:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----------+---------+-------+\n",
      "|client|auto|repair_date|indicator|  value|\n",
      "+------+----+-----------+---------+-------+\n",
      "|    c1|  a1|       2022|    level|   good|\n",
      "|    c1|  a1|       2022| velocity|     90|\n",
      "|    c1|  a1|       2023|    level|regular|\n",
      "|    c1|  a1|       2023| velocity|     80|\n",
      "|    c1|  a1|       2024|    level|  wrong|\n",
      "|    c1|  a1|       2024| velocity|     70|\n",
      "|    c2|  a1|       2022|    level|   good|\n",
      "|    c2|  a1|       2022| velocity|     90|\n",
      "|    c2|  a1|       2023|    level|  wrong|\n",
      "|    c2|  a1|       2023| velocity|     50|\n",
      "|    c2|  a2|       2024|    level|   good|\n",
      "|    c2|  a2|       2024| velocity|     80|\n",
      "+------+----+-----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:41:13.082004Z",
     "start_time": "2024-09-26T16:41:10.398351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "auto_repair.createOrReplaceTempView(\"auto_repair\")\n",
    "spark.sql('''\n",
    "with cte_1 as (\n",
    "select\n",
    "    *,\n",
    "    lead(value) over (partition by repair_date order by (select null)) as velocity\n",
    "from\n",
    "    auto_repair),\n",
    "cte_2 as (\n",
    "select\n",
    "    value,\n",
    "    velocity,\n",
    "    count(*) as cnt\n",
    "from\n",
    "    cte_1\n",
    "where\n",
    "    velocity REGEXP '^[0-9]+$'\n",
    "group by\n",
    "    value, velocity),\n",
    "cte_3 as (\n",
    "select\n",
    "    *,\n",
    "    case\n",
    "        when value = 'good' and velocity = 90 then cnt else 0\n",
    "    end as good_1,\n",
    "    case\n",
    "        when value = 'good' and velocity = 80 then cnt else 0\n",
    "    end as good_2,\n",
    "    case\n",
    "        when value = 'good' and velocity = 70 then cnt else 0\n",
    "    end as good_3,\n",
    "    case\n",
    "        when value = 'good' and velocity = 50 then cnt else 0\n",
    "    end as good_4,\n",
    "    case\n",
    "        when value = 'wrong' and velocity = 50 then cnt else 0\n",
    "    end as wrong_1,\n",
    "    case\n",
    "        when value = 'wrong' and velocity = 70 then cnt else 0\n",
    "    end as wrong_2,\n",
    "    case\n",
    "        when value = 'regular' and velocity = 80 then cnt else 0\n",
    "    end as regular\n",
    "from\n",
    "    cte_2)\n",
    "select\n",
    "    velocity,\n",
    "    sum(good_1+good_2+good_3+good_4) as good,\n",
    "    sum(wrong_1+wrong_2) as wrong,\n",
    "    sum(regular) as regular\n",
    "from\n",
    "    cte_3\n",
    "group by\n",
    "    velocity\n",
    "order by\n",
    "    velocity\n",
    "''').show()"
   ],
   "id": "de02c18075909536",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----+-------+\n",
      "|velocity|good|wrong|regular|\n",
      "+--------+----+-----+-------+\n",
      "|      50|   0|    1|      0|\n",
      "|      70|   0|    1|      0|\n",
      "|      80|   1|    0|      1|\n",
      "|      90|   2|    0|      0|\n",
      "+--------+----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e54a53cd4c8b5ef8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
